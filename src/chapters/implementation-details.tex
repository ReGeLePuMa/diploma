\chapter{Implementation Details}
\label{chap:implementation-details}

\section{Overview}

As stated previously, the implementation is written in Go, a statically typed, compiled language designed for simplicity and efficiency. The choice of Go was influenced by its 
support for both low-level systems programming and high-level programming paradigms, making it suitable for building robust and efficient applications.
Additionally, Go is one of the faster languages in terms of execution speed and features a rich ecosystem of community-built modules \cite{go-pkg}, which can be leveraged to speed up development.

\fig[scale=0.45]{src/img/language-speed.jpeg}{img:language-speed}{Language execution speed comparison \cite{benjdd}}

\pagebreak
\section{Project Structure}

The project is organized into multiple packages, each serving a specific purpose, as outlined in the architectural section.

\lstset{language=bash,caption=Project Structure,label=lst:project-structure}
\begin{lstlisting}
    .
	|-- binary_search
	  |-- binary_search.go
    |-- cmd
      |-- main.go
    |-- dockerminimizer
    |-- dockerminimizer.go
    |-- go.mod
    |-- go.sum
    |-- install.sh
    |-- ldd
      |-- ldd.go
    |-- logger
      |-- logger.go
    |-- preprocess
      |-- preprocess.go
    |-- README.md
    |-- strace
      |-- strace.go
    |-- types
      |-- types.go
    |-- utils
      |-- utils.go
\end{lstlisting}

The executable created is \textit{dockerminimizer}, which takes the following arguments:

\begin{itemize}
    \item \textbf{--file}: The path to the Dockerfile to be minimized.
    \item \textbf{--image}: The name of an images from a Docker registry to be minimized
    \item \textbf{--max_limit}: How many times should the binary search procedure be run for
    \item \textbf{--debug}: Enable logging of actions 
    \item \textbf{--timeout}: How long should the minimal Docker container run before being declared as successful
    \item \textbf{--strace_path}: The path to a statically-linked strace binary
    \item \textbf{--binary_search}: Decide whether to continue the minimization process with a binary search if dynamic analysis failed
\end{itemize}

\section{Preprocessing}

After the arguments are parsed, the program will create a temporary directory where the necessary files for the minimal Dockerfile will be stored under 
\textit{~/.dockerminimizer/<unique_id>}. The unique ID is generated using a \textit{MD5} hash of the current timestamp in order to ensure the uniqueness of the directory name should
multiple instances of the program be run at the same time.
The following steps are the building of the Docker image and the extraction of its filesystem to the directory. 
There are multiple ways of extracting a Docker container's filesystem \cite{extract-filesystem}, but the one we want to use is the one that preserves the original filesystem, without
adding any additional files. This can be achieved by passing the \textit{-o} flag to the build command, which will extract the unmodified filesystem to a desired location and type (the one
we are interested in is tarball). 
The docker image is then parsed for its metadata, which is stored in a JSON file, which contains information about the image's environment variables, entrypoint, command, exposed ports and user.
This metadata is stored as a custom type defined in the \textit{types}.

\lstset{language=Go,caption=Go DockerConfig structure,label=lst:metadata-type}
\begin{lstlisting}
    type DockerConfig struct {
	User         string                    `json:"User"`
	ExposedPorts map[string]map[string]any `json:"ExposedPorts"`
	Env          []string                  `json:"Env"`
	Cmd          []string                  `json:"Cmd"`
	WorkingDir   string                    `json:"WorkingDir"`
	Entrypoint   []string                  `json:"Entrypoint"`
}
\end{lstlisting}

This is then used to create a template Dockerfile, which serves as a base for all the other Dockerfiles that will be generated.

\lstset{language=Dockerfile,caption=Dockerfile.minimal.template,label=lst:template-minimal-dockerfile}
\begin{lstlisting}
FROM node:20.9.0 as builder

WORKDIR /app

COPY app.js /app/app.js
COPY package.json /app/package.json
COPY package-lock.json /app/package-lock.json

RUN npm install
EXPOSE 3000
CMD ["node", "app.js"]


FROM scratch

ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
ENV NODE_VERSION=20.9.0
ENV YARN_VERSION=1.22.19
WORKDIR /app
EXPOSE 3000/tcp
ENTRYPOINT ["docker-entrypoint.sh"]
CMD ["node", "app.js"]
\end{lstlisting}

This is then used to create the Dockerfile containing only the entrypoint/command files in order to check if it is already a minimal Dockerfile.
Should it not be the case, the program will proceed to the static analysis phase.

\section{Static Analysis}
In the static analysis phase, the base Dockerfile default running command (i.e first entrypoint/command in the image configuration) is run through \textit{ldd} in order to determine 
the libraries that are required for the program to run. In order to ensure that ldd finds the correct libraries, the command is run utilizing \textit{chroot} in the context of the 
extracted filesystem.
The output of the command is then parsed and stored into two dictionaries, one for the normal files and one for the symbolic links, which need to be resolved to the actual files in order to 
be copied accurately into the Dockerfile.

\lstset{language=Go,caption=ldd parser function,label=lst:ldd-parser}
\begin{lstlisting}
    func ParseOutput(output []byte, rootfsPath string) (map[string][]string, map[string]string) {
        files := make(map[string][]string)
        symLinks := make(map[string]string)
        scanner := bufio.NewScanner(bytes.NewReader(output))
        for scanner.Scan() {
            line := scanner.Text()
            if strings.Contains(line, "=>") {
                parts := strings.Split(line, "=>")
                lib := strings.Split(strings.TrimSpace(parts[1]), " ")[0]
    
                if strings.Contains(lib, "not found") {
                    continue
                }
                utils.AddFilesToDockerfile(lib, files, symLinks, rootfsPath)
    
            } else if strings.Contains(line, "not found") {
                continue
            } else {
                lib := strings.Split(strings.TrimSpace(line), " ")[0]
                utils.AddFilesToDockerfile(lib, files, symLinks, rootfsPath)
            }
        }
        return files, symLinks
    }
\end{lstlisting}

The reason why we are storing the files in a dictionary where the key is the directory name is due to 
the Docker's limit of 127 build stages, meaning that there can be at most 127 \textit{COPY} commands in the minimal Dockerfile.
Fortunately, we can circumvent this by putting all the files that belong to the same directory in the same \textit{COPY} command.

The way this is achieved is by utilizing Go's \textit{filepath} package, which allows for easy manipulation of file paths.

\lstset{language=Go,caption=File util functions,label=lst:file-functions}
\begin{lstlisting}
func RealPath(path string) string {
	realPath, _ := filepath.Abs(path)
	return filepath.Clean(realPath)
}

func CheckIfFileExists(file string, envPath string) bool {
	info, err := os.Stat(envPath + "/" + file)
	return !os.IsNotExist(err) && !info.IsDir()
}

func CheckIfSymbolicLink(file string, envPath string) bool {
	info, err := os.Lstat(envPath + "/" + file)
	if err != nil {
		return false
	}
	return info.Mode()&os.ModeSymlink != 0
}

func ReadSymbolicLink(file string, envPath string) string {
	link, _ := os.Readlink(envPath + "/" + file)
	resolved := link
	if !filepath.IsAbs(link) {
		resolved = filepath.Join(filepath.Dir(envPath+"/"+file), link)
	}
	return strings.TrimPrefix(resolved, envPath)
}

func AddFilesToDockerfile(file string, files map[string][]string, symLinks map[string]string, rootfsPath string) {
	file = RealPath(file)
	if CheckIfFileExists(file, rootfsPath) {
		if CheckIfSymbolicLink(file, rootfsPath) {
			symLinks[file] = ReadSymbolicLink(file, rootfsPath)
		} else {
			files[filepath.Dir(file)] = AppendIfMissing(files[filepath.Dir(file)], file)
		}
	}
}
\end{lstlisting}

With the two dictionaries populated, they are used to create the Dockerfile for this stage, by adding the \textit{COPY} commands for each directory to 
the template Dockerfile, which is then tested to see if it can build and run the container successfully. If it it not, the next step is the dynamic analysis of the command.

\section{Dynamic Analysis}

\fig[scale=0.5]{src/img/bind-mount.pdf}{img:bind-mount}{Dynamic analysis using strace to determine the files used by the command}

For this stage, we attach a \textit{strace} probe to the docker running command, in order to determine the files used and processes that are spawned by the command.
In order to do this, we utilize Docker's volumes to mount a statically-linked \textit{strace} binary into the container, as well as a logfile to save the output of the command.
The reason for using a statically-linked binary is so that the libraries used by strace don't interfere with the libraries used by the command.
To determine the runtime dependencies of the command, we will trace the \textit{open} and \textit{exec} familly of syscalls \cite{identify-dependencies} as well as passing the \textit{-f} flag
to trace the child processes as well.

\lstset{language=Go,caption=Strace output parser,label=lst:strace-parser}
\begin{lstlisting}
    func parseOutput(output string, syscalls []string, files map[string][]string, symLinks map[string]string, envPath string) {
        regexes := make(map[string]*regexp.Regexp)
        for _, syscall := range syscalls {
            regexes[syscall] = regexp.MustCompile(syscall + `\([^"]*?"([^"]+)"`)
        }
        for line := range strings.SplitSeq(output, "\n") {
            for _, syscall := range syscalls {
                if regexes[syscall].MatchString(line) {
                    match := regexes[syscall].FindStringSubmatch(line)
                    if len(match) > 1 {
                        utils.AddFilesToDockerfile(match[1], files, symLinks, envPath+"/rootfs")
                    }
                    break
                }
            }
        }
    }
\end{lstlisting}

Parsing the output is made trivial by using regexes to match the syscalls and extract the first text between \textit{""}.

A problem that was quickly encountered was that in dealing with commands that are run via shebangs.
A shebang is a special comment that is used to indicate the interpreter that should be used to run the script, and are always 
put at the top of the file, starting with the characters \textit{\#!}.
When stracing the command, only the command itself is being shown as executed, and not the shebang. This is due 
to how the operating system handles the shebang. Finding the interpreter is done internally by the kernel in the
\textit{load_script} function, which is not visible to the strace command \cite{demystifying-shebang}. 
To fix this and make this step more robust, we firstly determine if a shebang is present in the base command, and afterwards 
we follow the minimization steps of static and dynamic analysis on the interpreter as well.

\lstset{language=Go,caption=Parsing and determening shebang dependencies utility,label=lst:shebang-parser}
\begin{lstlisting}
    func getSheBang(command string, rootfsPath string) string {
	file, err := os.Open(rootfsPath + "/" + command)
	if err != nil {
		log.Error("Failed to open file:", command)
		return ""
	}
	defer file.Close()
	var firstLine []byte
	buf := make([]byte, 1)
	for {
		n, err := file.Read(buf)
		if n > 0 {
			if buf[0] == '\n' {
				break
			}
			firstLine = append(firstLine, buf[0])
		}
		if err != nil {
			break
		}
	}
	return string(firstLine)
}

func parseShebang(imageName string, containerName string, syscalls []string,
	files map[string][]string, symLinks map[string]string, envPath string, metadata types.DockerConfig, timeout int) (map[string][]string, map[string]string) {
	command := utils.GetContainerCommand(imageName, envPath, metadata)
	hasSudo := utils.HasSudo()
	shebang := getSheBang(command, envPath+"/rootfs")
	regex := regexp.MustCompile(`^#!\s*([^\s]+)`)
	if !regex.MatchString(shebang) {
		log.Error("Failed to find shebang in file:", command)
		return files, symLinks
	}
	match := regex.FindStringSubmatch(shebang)
	if len(match) < 2 {
		log.Error("Failed to find interpreter in shebang:", command)
		return files, symLinks
	}
	interpreter := match[1]
	lddCommand := hasSudo + " chroot " + envPath + "/rootfs ldd " + interpreter
	log.Info("Running command:", lddCommand)
	lddOutput, err := exec.Command("sh", "-c", lddCommand).CombinedOutput()
	if err != nil {
		log.Error("Failed to run ldd command\n" + err.Error())
	}
	files, symLinks = ldd.ParseOutput(lddOutput, envPath+"/rootfs")

	output := getStraceOutput(imageName, envPath+"/strace", envPath+"/log.txt", syscalls,
		containerName, interpreter, envPath, metadata, timeout)
	parseOutput(output, syscalls, files, symLinks, envPath)
	return files, symLinks
}
\end{lstlisting}

Finally, we create the Dockerfile for this stage and validate it like in the static analysis phase.

\section{Binary Search}

Should the dynamic analysis fail to produce the minimal Dockerfile, we proceed to the \textit{binary search} phase, so called because of 
the splitting in half of the search space, in our case being the files of the Docker image.
The first step of this phase is to create into memory the filesystem structure as a dictionary, aptly named \textit{unusedFiles}.

\lstset{language=Go,caption=Filesystem parsing,label=lst:filesystem-parsing}
\begin{lstlisting}
func parseFilesystem(rootfsPath string) (map[string][]string,
	map[string][]string, error) {
	info, err := os.Stat(rootfsPath)
	if err != nil {
		log.Error("Error reading rootfs path:", err)
		return nil, nil, err
	}
	if !info.IsDir() {
		log.Error("Rootfs path is not a directory")
		return nil, nil, errors.New("rootfs path is not a directory")
	}

	usedFiles := make(map[string][]string)
	unusedFiles := make(map[string][]string)
	err = filepath.WalkDir(rootfsPath, func(path string, d fs.DirEntry, err error) error {
		if err != nil {
			log.Error("Error walking directory:", err)
			return err
		}
		relPath := strings.TrimPrefix(path, rootfsPath)
		if relPath == "" {
			relPath = "/"
		}
		unusedFiles[filepath.Dir(relPath)] = utils.AppendIfMissing(unusedFiles[filepath.Dir(relPath)], relPath)
		return nil
	})
	if err != nil {
		log.Error("Error walking directory:", err)
		return nil, nil, err
	}
	return usedFiles, unusedFiles, nil
}
\end{lstlisting}

For a maximum of \textit{MAX_LIMIT} or until the \textit{unusedFiles} dictionary is empty, we will run the following steps:
\begin{enumerate}
	\item Split the \textit{unusedFiles} dictionary in half, creating a new dictionary called \textit{usedFiles}
	\item Create a tarball from the \textit{usedFiles} dictionary and add it to the Dockerfile
	\item If the image runs successfully, the \textit{usedFiles} dictionary becomes \textit{unusedFiles} dictionary for the following iteration.
	\item If the image fails to run, repeat step 1.
\end{enumerate}

\lstset{language=Go,caption=Binary search function,label=lst:binary-search}
\begin{lstlisting}
func binarySearchStep(envPath string, context string, timeout int, step int, usedFiles map[string][]string,
	unusedFiles map[string][]string) (map[string][]string, map[string][]string, error) {
	for {
		if len(unusedFiles) == 0 {
			return nil, nil, errors.New("no unused files or symbolic links left to process")
		}
		usedFiles, unusedFiles = splitFilesystem(usedFiles, unusedFiles)
		filename := fmt.Sprintf("Dockerfile.minimal.binary_search.%d", step)
		tarFilename := fmt.Sprintf("%s/files.tar", envPath)
		if err := utils.BuildTarArchive(usedFiles, tarFilename, envPath); err != nil {
			log.Error("Error building tar archive:", err)
			return nil, nil, err
		}
		err := utils.AddTarToDockerfile(filename, "Dockerfile.minimal.template", envPath)
		if err != nil {
			log.Error("Error adding tar to Dockerfile:", err)
			return nil, nil, errors.New("error adding tar to Dockerfile")
		}
		utils.CopyFile(tarFilename, context+"/files.tar")
		err = utils.ValidateDockerfile(filename, envPath, context, timeout)
		os.Remove(context + "/files.tar")
		if err != nil {
			exec.Command("docker", "rmi", "-f", "dockerminimize-"+filepath.Base(envPath)+":"+fmt.Sprint(step)).Run()
		}
		if err == nil {
			log.Info("Binary search step ", step, " succeeded.")
			utils.CopyFile(envPath+"/files.tar", "files.tar")
			return make(map[string][]string), usedFiles, nil
		}
	}
}

func BinarySearch(envPath string, maxLimit int, context string, timeout int) error {
	log.Info("Starting binary search...")
	usedFiles, unusedFiles, err := parseFilesystem(envPath + "/rootfs")
	if err != nil {
		log.Error("Error parsing filesystem:", err)
		return errors.New("error parsing filesystem")
	}

	step := 0
	var lastErr error
	for step := 1; step <= maxLimit; step++ {
		log.Info("Binary search iteration:", step)
		usedFiles, unusedFiles, lastErr = binarySearchStep(envPath, context, timeout, step,
			usedFiles, unusedFiles)
		if lastErr != nil {
			break
		}
	}

	if lastErr != nil {
		log.Error("Binary search failed at step", step, "with error:", lastErr)
		return lastErr
	}

	if step > maxLimit {
		log.Info("Reached maximum limit of binary search iterations:", maxLimit)
		return errors.New("reached maximum limit of binary search iterations")
	}

	log.Info("Binary search completed successfully.")
	utils.CopyFile(envPath+"/files.tar", "files.tar")
	return nil
}

\end{lstlisting}

Splitting the \textit{unusedFiles} dictionary is done by simulating the randomness of a coin flip, where \textit{heads} or in our case \textit{0} means add the file to the ones
that will be used.
\lstset{language=Go,caption=Splitting the filesystem,label=lst:split-filesystem}
\begin{lstlisting}
	func splitFilesystem(usedFiles map[string][]string,
	unusedFiles map[string][]string) (map[string][]string, map[string][]string) {
	cpyUsedFiles, _ := deepcopy.Anything(usedFiles)
	cpyUnusedFiles, _ := deepcopy.Anything(unusedFiles)
	usedFiles, _ = cpyUsedFiles.(map[string][]string)
	unusedFiles, _ = cpyUnusedFiles.(map[string][]string)
	for dir, files := range unusedFiles {
		originalFiles := slices.Clone(files)
		for _, file := range originalFiles {
			flag, _ := rand.Int(rand.Reader, big.NewInt(2))
			ok := flag.Int64()
			if ok == 0 {
				usedFiles[dir] = utils.AppendIfMissing(usedFiles[dir], file)
				unusedFiles[dir] = utils.RemoveElement(unusedFiles[dir], file)
			}
		}
		if len(unusedFiles[dir]) == 0 {
			delete(unusedFiles, dir)
		}
	}
	return usedFiles, unusedFiles
}
\end{lstlisting}

This is done to ensure fairness in the selection of the files, assuming that all files are equally likely to be used by the application.

\fig[scale=0.5]{src/img/tarball-add.pdf}{img:tarball-add}{Adding a tarball to the Dockerfile \cite{docker-add-command}}[2pt]


In order to check if these are the files that will produce a running container, we need to add them to the Dockerfile and build the image. However, we cannot simply follow the 
same steps as in the static and dynamic analysis phases, since of the sheer number of files that have to be copied which run the risk of exceeding Docker's hard limit of \textit{127} image layers
\cite{docker-layers}. In order to circumvent this, we will create a tarball (tar archive) of the files used and add it to the Dockerfile using the \textit{ADD} command, which will automatically extract the files 
into the container's filesystem.


Finally, we validate the Dockerfile by building the image and running it to check if the command executes successfully. 